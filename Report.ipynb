{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Deep Reinforcement Learning Nanodegree (DRLND): Project 1 - Navigation\n",
    "\n",
    "I am using my COVID-19 imposed quarantine to expand my deep learning skills by completing the [*Deep Reinforcement Learning Nanodegree*](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) from [Udacity](https://www.udacity.com/). In this notebook I discuss my solution to the first DRLND project on navigation. For my solution I chose to implement the double deep Q-network algorithm with prioritized experience replay. My implementation combined insights from three seminal papers in the deep reinforcement learning literature.\n",
    "\n",
    "* [*Human-level control through deep reinforcement learning*](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) (Minh et al 2015).\n",
    "* [*Deep Reinforcement Learning with Double Q-Learning*](https://arxiv.org/abs/1509.06461) (Van Hasselt et al 2015)\n",
    "*  [*Prioritized Experience Replay*](https://arxiv.org/abs/1511.05952) (Schaul et al 2016)\n",
    "\n",
    "## Human-level control through deep reinforcement learning (Minh et al 2015)\n",
    "\n",
    "While Minh et al 2015 was not the first paper to use neural networks to approximate the action-value function, this paper was the first to demonstrate that the same neural network architecture could be trained in a computationally efficient manner to \"solve\" a large number or different tasks. The paper also contributed several practical \"tricks\" for getting deep neural networks to consistently converge during training. This was a non-trivial contribution as issues with training convergence had plaugued previous attempts to use neural networks as function approximators in reinforcement learning tasks and were blocking widespread adoption of deep learning techniques within the reinforcemnt learning community.\n",
    "\n",
    "Minh et al 2015 uses deep (convolutional) neural network to approximate the optimal action-value function\n",
    "\n",
    "$$ Q^*(s, a) = \\max_{\\pi} \\mathbb{E}\\Bigg[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k} | S_t=s, A_t=a, \\pi \\Bigg] $$\n",
    "\n",
    "which is the maximum sum of rewards $R_t$ discounted by $\\gamma$ at each time-step $t$ achievable by a behaviour policy $\\pi = P(a|s)$, after making an observation of the state $s$ and taking an action $a$. \n",
    "\n",
    "Prior to this seminal paper it was well known that standard reinforcement learning algorithms were unstable or even diverged when a non-linear function approximators such as a neural networks were used to represent the action-value function $Q$. Why? Several reasons.\n",
    "\n",
    "1. Correlations present in the sequence of observations of the state $s$. In reinforcement learning applications the sequence state observations is a time-series which will almost surely be auto-correlated. But surely this would also be true of any application of deep neural networks to model time series data so it is not obvious why this should be such a big deal.\n",
    "2. Small updates to $Q$ may significantly change the policy, $\\pi$ and therefore change the data distribution.\n",
    "3. Correlations between the action-values, $Q$, and the target values $R + \\gamma \\underset{a'}{\\max} Q(s', a')$\n",
    "\n",
    "The authors address these issues by using...\n",
    "\n",
    "* a biologically inspired mechanism they refer to as *experience replay* that randomizes over the data which removes correlations in the sequence of observations of the state $s$ and smoothes over changes in the data distribution (issues 1 and 2 above).\n",
    "* an iterative update rule that adjusts the action-values, $Q$, towards target values, $Q'$ that are only periodically updated thereby reducing correlations with the target (issue 3 above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximating the action-value function, $Q(s,a)$\n",
    "\n",
    "There are several possible ways of approximating the action-value function $Q$ using a neural network. The only input to the DQN architecture is the state representation and the output layer has a separate output for each possible action. The output units correspond to the predicted $Q$-values of the individual actions for the input state. A representaion of the DQN architecture from the paper is reproduced in the figure below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=assets/images/q-network-architecture.jpg width=1000>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the neural network consists of an 84 x 84 x 4 image produced by some preprocessing map $\\phi$. The network has four hidden layers:\n",
    "\n",
    "* Convolutional layer with 32 filters (each of which uses an 8 x 8 kernel and a stride of 4) and a ReLU activation function. \n",
    "* Convolutional layer with 64 filters (each of which using a 4 x 4 kernel with stride of 2) and a ReLU activation function. \n",
    "* Convolutional layer with 64 filters (each of which uses a 3 x 3 kernel and a stride of 1) and a ReLU activation function. \n",
    "* Fully-connected (i.e., dense) layer with 512 neurons followed by a ReLU activation function.\n",
    "\n",
    "The output layer is another fully-connected layer with a single output for each action. A PyTorch implementation of the DQN architecture would look something like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "DeepQNetwork = nn.Module\n",
    "DeepQNetworkFn = typing.Callable[[], DeepQNetwork]\n",
    "\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, f):\n",
    "        super().__init__()\n",
    "        self._f = f\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self._f(X)\n",
    "\n",
    "\n",
    "def make_deep_q_network_fn(action_size: int) -> DeepQNetworkFn:\n",
    "    \n",
    "    def deep_q_network_fn() -> DeepQNetwork:\n",
    "        q_network = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            LambdaLayer(lambda tensor: tensor.view(tensor.size(0), -1)),\n",
    "            nn.Linear(in_features=25024, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=action_size)\n",
    "        )\n",
    "        return q_network\n",
    "    \n",
    "    return deep_q_network_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, for this project at least, I am not learning directly from pixels/images but am rather \n",
    "working with a preprocessed representation of the environment's state I decided to implement a three \n",
    "layer dense neural network. I use an outer function to close over the parameters defining the \n",
    "network architecture: the number of states and the number of actions are determined by the \n",
    "environment so the the only hyperparameter is the number of hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_deep_q_network_fn(number_states: int,\n",
    "                           number_actions: int,\n",
    "                           number_hidden_units: int) -> agents.DeepQNetworkFn:\n",
    "    \"\"\"Create a function that returns a DeepQNetwork with appropriate input and output shapes.\"\"\"\n",
    "    \n",
    "    def deep_q_network_fn() -> agents.DeepQNetwork:\n",
    "        deep_q_network = nn.Sequential(\n",
    "            nn.Linear(in_features=number_states, out_features=number_hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=number_hidden_units, out_features=number_hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=number_hidden_units, out_features=number_actions)\n",
    "        )\n",
    "        return deep_q_network\n",
    "    \n",
    "    return deep_q_network_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning with Double Q-Learning (Van Hasselt et al 2015)\n",
    "\n",
    "The Van Hasselt et al 2015 paper makes several important contributions. \n",
    "\n",
    "1. Demonstration of how Q-learning can be overly optimistic in large-scale, even \n",
    "   deterministic, problems due to the inherent estimation errors of learning. \n",
    "2. Demonstration that overestimations are more common and severe in practice than previously \n",
    "   acknowledged. \n",
    "3. Implementation of Double Q-learning called Double DQN that extends, with minor \n",
    "   modifications, the popular DQN algorithm and that can be used at scale to successfully \n",
    "   reduce overestimations with the result being more stable and reliable learning.\n",
    "4. Demonstation that Double DQN finds better policies by obtaining new state-of-the-art \n",
    "   results on the Atari 2600 dataset.\n",
    "   \n",
    "### Q-learning overestimates Q-values\n",
    "\n",
    "No matter what type of function approximation scheme used to approximate the action-value function \n",
    "$Q$ there will always be approximation error. The presence of the max operator in the \n",
    "[Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation) used to compute the $Q$-values \n",
    "means that the approximate $Q$-values will almost always be strictly greater than the corresponding \n",
    "$Q$ values from the true action-value function (i.e., the approximation errors will almost always \n",
    "be positive). This potentially significant source of bias can impede learning and is often \n",
    "exacerbated by the use of flexible, non-linear function approximators such as neural networks. \n",
    "\n",
    "Double Q-learning addresses these issues by explicitly separating action selection from action \n",
    "evaluation which allows each step to use a different function approximator resulting in a better \n",
    "overall approximation of the action-value function. Figure 2 (with caption) below, which is taken \n",
    "from Van Hasselt et al 2015, summarizes these ideas. See the \n",
    "[paper](https://arxiv.org/pdf/1509.06461.pdf) for more details.\n",
    "\n",
    "<center>\n",
    "    <img src=assets/images/double-dqn-figure-2.png width=1000>\n",
    "</center>\n",
    "    \n",
    "### Implementing the double deep Q-network update\n",
    "\n",
    "The key idea behind Double Q-learning is to reduce overestimations of Q-values by separating the selection of actions from the evaluation of those actions so that a different Q-network can be used in each step. When applying Double Q-learning to extend the DQN algorithm one can use the online Q-network, $Q(S, a; \\theta)$, to select the actions and then the target Q-network, $Q(S, a; \\theta^{-})$, to evaluate the selected actions.\n",
    "\n",
    "Before implement the Double DQN algorithm, I am going to re-implement the Q-learning update from the DQN algorithm in a way that explicitly separates action selection from action evaluation. Once I have implemented this new version of Q-learning, implementing the Double DQN algorithm will be much easier. Formally separating action selection from action evaluation involves re-writing the Q-learning Bellman equation as follows.\n",
    "\n",
    "$$ Y_t^{DQN} = R_{t+1} + \\gamma Q\\big(S_{t+1}, \\underset{a}{\\mathrm{argmax}}\\ Q(S_{t+1}, a; \\theta_t); \\theta_t\\big) $$\n",
    "\n",
    "In Python this can be implemented as three separate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Action = int\n",
    "Actions = torch.Tensor\n",
    "Dones = torch.Tensor\n",
    "QValues = torch.Tensor\n",
    "Rewards = torch.Tensor\n",
    "States = torch.Tensor\n",
    "\n",
    "\n",
    "def select_greedy_action(states: States, q_network: DeepQNetwork) -> Action:\n",
    "    \"\"\"Select the greedy action for the current state given some Q-network.\"\"\"\n",
    "    _, actions = q_network(states).max(dim=1, keepdim=True)\n",
    "    action = (actions.cpu()\n",
    "                     .item())\n",
    "    return action\n",
    "\n",
    "\n",
    "def select_greedy_actions(states: torch.Tensor, q_network: DeepQNetwork) -> Actions:\n",
    "    \"\"\"Select the greedy actions for the current states given some Q-network.\"\"\"\n",
    "    _, actions = q_network(states).max(dim=1, keepdim=True)\n",
    "    return actions\n",
    "\n",
    "\n",
    "def evaluate_selected_actions(states: States,\n",
    "                              actions: Actions,\n",
    "                              rewards: Rewards,\n",
    "                              dones: Dones,\n",
    "                              gamma: float,\n",
    "                              q_network: DeepQNetwork) -> QValues:\n",
    "    \"\"\"Compute the Q-values by evaluating the actions given the current states and Q-network.\"\"\"\n",
    "    next_q_values = q_network(states).gather(dim=1, index=actions)        \n",
    "    q_values = rewards + (gamma * next_q_values * (1 - dones))\n",
    "    return q_values\n",
    "\n",
    "\n",
    "def q_learning_update(states: States,\n",
    "                      rewards: Rewards,\n",
    "                      dones: Dones,\n",
    "                      gamma: float,\n",
    "                      q_network: DeepQNetwork) -> QValues:\n",
    "    \"\"\"Q-Learning uses a single Q-network to select and evaluate actions.\"\"\"\n",
    "    actions = select_greedy_actions(states, q1)\n",
    "    q_values = evaluate_selected_actions(states, actions, rewards, dones, gamma, q1)\n",
    "    return q_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here it is straight forward to implement the Double DQN algorithm. All I need is a second action-value function. The target network in the DQN architecture provides a natural candidate for the second action-value function. Hasselt et al 2015 suggest using the online Q-network to select the greedy policy actions before using the target Q-network to estimate the value of the selected actions. Once again here are the maths...\n",
    "\n",
    "$$ Y_t^{DoubleDQN} = R_{t+1} + \\gamma Q\\big(S_{t+1}, \\underset{a}{\\mathrm{argmax}}\\ Q(S_{t+1}, a; \\theta_t), \\theta_t^{-}\\big) $$\n",
    "\n",
    "...and here is the the Python implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_q_learning_update(states: States,\n",
    "                             rewards: Rewards,\n",
    "                             dones: Dones,\n",
    "                             gamma: float,\n",
    "                             q1: DeepQNetwork,\n",
    "                             q2: DeepQNetwork) -> QValues:\n",
    "    \"\"\"Double Q-Learning uses q1 to select actions and q2 to evaluate the selected actions.\"\"\"\n",
    "    actions = select_greedy_actions(states, q1)\n",
    "    q_values = evaluate_selected_actions(states, actions, rewards, dones, gamma, q2)\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the function `double_q_learning_update` is almost identical to the `q_learning_update` function above: all that is needed is to introduce a second Q-network parameter, `q2`, to the function. This second Q-network will be use to evaluate the actions chosen using the original Q-network parameter, now called `q1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized Experience Replay (Schaul et al 2016)\n",
    "\n",
    "In this section I will discuss implementation of an important enhancement of the experience replay \n",
    "idea from [*Prioritized Experience Replay*](https://arxiv.org/abs/1511.05952) (Schaul et al 2016).\n",
    "\n",
    "The following quote from the paper nicely summarizes the key idea.\n",
    "\n",
    "> Experience replay liberates online learning agents from processing transitions in the exact order\n",
    "they are experienced. Prioritized replay further liberates agents from considering transitions with\n",
    "the same frequency that they are experienced.\n",
    "...\n",
    "In particular, we propose to more frequently replay transitions with high expected learning progress,\n",
    "as measured by the magnitude of their temporal-difference (TD) error. This prioritization can lead\n",
    "to a loss of diversity, which we alleviate with stochastic prioritization, and introduce bias, which\n",
    "we correct with importance sampling.\n",
    "\n",
    "Using an experience replay buffer naturally leads to two issues that need to be addressed. \n",
    "\n",
    "1. Which experiences should the agent store in the replay buffer?\n",
    "2. Which experiences should the agent replay from the buffer in order to learn efficiently?\n",
    "\n",
    "Schaul et al 2016 take the contents of the replay buffer more or less as given and focus solely on \n",
    "answering the second question. That is, the paper focuses on developing a procedure for making the \n",
    "most effective use of the experience replay buffer for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritization using the temporal-difference (TD) error term\n",
    "\n",
    "You can't prioritize experiences for learning unless you can measure the importance of each \n",
    "experience in the learning process. The ideal criterion would be the amount that RL agent can \n",
    "learn from experience given the current state (i.e., the expected learning value of the \n",
    "experience). \n",
    "\n",
    "Unfortunately such an ideal criterion is not directly measurable. However, a reasonable proxy is \n",
    "the magnitude of an experience’s temporal-difference (TD) error $\\delta_i$. The TD-error \n",
    "indicates how \"surprising\" or \"unexpected\" the experience is given the current state of the RL \n",
    "agent. Using the TD-error term to prioritize experiences for replay is particularly suitable for \n",
    "incremental, online RL algorithms, such as \n",
    "[SARSA](https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action) \n",
    "or [Q-learning](https://en.wikipedia.org/wiki/Q-learning), \n",
    "as these algorithms already compute the TD-error and update the parameters proportionally.\n",
    "\n",
    "Using the notation developed above the TD-error term can be written as follows.\n",
    "\n",
    "$$ \\delta_{i,t} = R_{i,t+1} + \\gamma Q\\big(S_{i,t+1}, \\underset{a}{\\mathrm{argmax}}\\ Q(S_{i,t+1}, a; \\theta_t); \\theta^{-}_t\\big) - Q(S_{i,t}, a_{i,t}; \\theta_t\\big)$$\n",
    "\n",
    "In the cell below I define a function for computing the TD-error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDErrors = torch.Tensor\n",
    "\n",
    "\n",
    "def double_q_learning_error(states: States,\n",
    "                            actions: Actions,\n",
    "                            rewards: Rewards,\n",
    "                            next_states: States,\n",
    "                            dones: Dones,\n",
    "                            gamma: float,\n",
    "                            q1: DeepQNetwork,\n",
    "                            q2: DeepQNetwork) -> TDErrors:\n",
    "    expected_q_values = double_q_learning_update(next_states, rewards, dones, gamma, q1, q2)\n",
    "    q_values = q1(states).gather(dim=1, index=actions)\n",
    "    delta = expected_q_values - q_values\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have defined a measurable criterion by which an RL agent can prioritize its \n",
    "experiences, I can move on to discussing the major contribution of the Schaul et al 2016 paper \n",
    "which was an efficient procedure for randomly sampling and replaying prioritized experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic prioritization\n",
    "\n",
    "Schaul et al 2016 introduce a stochastic sampling method that interpolates between pure greedy \n",
    "experience prioritization (i.e., always sampling the highest priority experiences) and uniform \n",
    "random sampling of experience. The probability of sampling experience $i$ is defined as follows.\n",
    "\n",
    "$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_{j=0}^{N} p_j^{\\alpha}} $$\n",
    "\n",
    "where $p_i > 0$ is the priority of transition $i$. The exponent $\\alpha$ determines how much \n",
    "prioritization is used, with $\\alpha = 0$ corresponding to the uniform random sampling case. Note \n",
    "that the probability of being sampled is monotonic in an experience’s priority while guaranteeing \n",
    "a non-zero probability for the lowest-priority experience.\n",
    "\n",
    "### Correcting for sampling bias\n",
    "\n",
    "Estimation of the expected value from stochastic updates relies on those updates being drawn from \n",
    "the same underlying distribution whose expectation you wish to estimate. Prioritized experience \n",
    "replay introduces a form of sampling bias that changes the underlying distribution (whose \n",
    "expectation needs to be estimated) in an uncontrolled fashion. When the underlying distribution \n",
    "changes, the solution to which the algorithm will converge also changes (even if the policy and \n",
    "state distribution are fixed). In order for the algorithm to converge properly, the bias \n",
    "introduced by the prioritized experience replay procedure needs to be corrected.\n",
    "\n",
    "Schaul et al 2016 correct for this bias using an importance sampling scheme that computes a weight \n",
    "for each sampled experience that can be used when computing the loss for that sample.\n",
    "\n",
    "$$ w_i = \\left(\\frac{1}{N}\\frac{1}{P(i)}\\right)^\\beta $$\n",
    "\n",
    "The hyperparameter $\\beta \\ge 0$ controls how strongly to correct for the bias: $\\beta=0$ implies \n",
    "no correction; $\\beta=1$ fully compensates for the bias. For stability reasons, since these \n",
    "importance sampling weights are included in the loss, they are be normalized by $\\max_i\\ w_i$.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "There are two implementation details of my `PrioritizedExperienceReplayBuffer` that I think are \n",
    "noteworthly.\n",
    "\n",
    "* Use of a `nametuple` as the container type for an `Experience`. Modeling and `Experience` as a \n",
    "  `namedtuple` allows for a collection of `Experience` instances to be zipped which facilitates \n",
    "  creation of `torch.Tensor` instances for states, actions, rewards, etc. \n",
    "* Use instead of using a [fixed-length, double-ended queue](https://docs.python.org/3.8/library/collections.html#collections.deque) \n",
    "  as the underlying data structure for storing prioritized experiences, I use a NumPy \n",
    "  [structured array](https://docs.scipy.org/doc/numpy/user/basics.rec.html) to store \n",
    "  priority-experience tuples which substantially improves the algorithm performance.\n",
    "\n",
    "The full implementation if provided below for reference and can also be found in [`src/replay_buffers.py`](src/replay_buffers.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "BetaAnnealingSchedule = typing.Optional[typing.Callable[[int, float], float]]\n",
    "RandomState = typing.Optional[np.random.RandomState]\n",
    "\n",
    "\n",
    "def sampling_probabilities(priorities: np.ndarray, alpha: float):\n",
    "    \"\"\"Sampling probability is increasing function of priority\"\"\"\n",
    "    return priorities**alpha / np.sum(priorities**alpha)\n",
    "    \n",
    "\n",
    "def sampling_weights(probabilities: np.ndarray, beta: float, normalize: bool):\n",
    "    \"\"\"Importance sampling weights correct for sampling bias introduced by prioritization.\"\"\"\n",
    "    n = probabilities.size\n",
    "    weights = (n * probabilities)**-beta\n",
    "    if normalize:\n",
    "        weights = weights / weights.max()\n",
    "    return weights\n",
    "\n",
    "\n",
    "_field_names = [\n",
    "    \"state\",\n",
    "    \"action\",\n",
    "    \"reward\",\n",
    "    \"next_state\",\n",
    "    \"done\"\n",
    "]\n",
    "Experience = collections.namedtuple(\"Experience\", field_names=_field_names)\n",
    "\n",
    "\n",
    "class PrioritizedExperienceReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store priority, Experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 maximum_size: int,\n",
    "                 alpha: float = 0.0,\n",
    "                 beta_annealing_schedule: BetaAnnealingSchedule = None,\n",
    "                 initial_beta: float = 0.0,\n",
    "                 random_state: RandomState = None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize a PrioritizedExperienceReplayBuffer object.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        maximum_size (int): maximum size of buffer\n",
    "        alpha (float): Strength of prioritized sampling. Default to 0.0 (i.e., uniform sampling).\n",
    "        beta_annealing_schedule (BetaAnnealingSchedule): function that takes an episode number and \n",
    "            an initial value for beta and returns the current value of beta.\n",
    "        random_state (np.random.RandomState): random number generator.\n",
    "        \n",
    "        \"\"\"\n",
    "        self._maximum_size = maximum_size\n",
    "        self._current_size = 0 # current number of prioritized experience tuples in buffer\n",
    "        _dtype = [(\"priority\", np.float32), (\"experience\", Experience)]\n",
    "        self._buffer = np.empty(self._maximum_size, _dtype)\n",
    "        self._alpha = alpha\n",
    "        self._initial_beta = initial_beta\n",
    "        \n",
    "        if beta_annealing_schedule is None:\n",
    "            self._beta_annealing_schedule = lambda n: self._initial_beta\n",
    "        else:\n",
    "            self._beta_annealing_schedule = lambda n: beta_annealing_schedule(n, self._initial_beta)\n",
    "\n",
    "        self._random_state = np.random.RandomState() if random_state is None else random_state\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Current number of prioritized experience tuple stored in buffer.\"\"\"\n",
    "        return self._current_size\n",
    "\n",
    "    @property\n",
    "    def alpha(self):\n",
    "        \"\"\"Strength of prioritized sampling.\"\"\"\n",
    "        return self._alpha\n",
    "    \n",
    "    @property\n",
    "    def maximum_size(self) -> int:\n",
    "        \"\"\"Maximum number of prioritized experience tuples stored in buffer.\"\"\"\n",
    "        return self._maximum_size\n",
    "    \n",
    "    @property\n",
    "    def initial_beta(self):\n",
    "        \"\"\"Initial strength for sampling correction.\"\"\"\n",
    "        return self._initial_beta\n",
    "\n",
    "    def add(self, experience: Experience) -> None:\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        priority = 1.0 if self.is_empty() else self._buffer[\"priority\"].max()\n",
    "        if self.is_full():\n",
    "            if priority > self._buffer[\"priority\"].min():\n",
    "                idx = self._buffer[\"priority\"].argmin()\n",
    "                self._buffer[idx] = (priority, experience)\n",
    "            else:\n",
    "                pass # low priority experiences should not be included in buffer\n",
    "        else:\n",
    "            self._buffer[self._current_size] = (priority, experience)\n",
    "            self._current_size += 1\n",
    "\n",
    "    def is_empty(self) -> bool:\n",
    "        \"\"\"True if the buffer is empty; False otherwise.\"\"\"\n",
    "        return self._current_size == 0\n",
    "    \n",
    "    def is_full(self) -> bool:\n",
    "        \"\"\"True if the buffer is full; False otherwise.\"\"\"\n",
    "        return self._current_size == self._maximum_size\n",
    "    \n",
    "    def sample(self, batch_size: int, episode_number: int) -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Sample a batch of experiences from memory.\"\"\"\n",
    "        # use sampling scheme to determine which experiences to use for learning\n",
    "        ps = self._buffer[:self._current_size][\"priority\"]\n",
    "        sampling_probs = sampling_probabilities(ps, self._alpha)\n",
    "        \n",
    "        # use sampling probabilities to compute sampling weights\n",
    "        beta = self._beta_annealing_schedule(episode_number)\n",
    "        weights = sampling_weights(sampling_probs, beta, normalize=True)\n",
    "        \n",
    "        # randomly sample indicies corresponding to priority, experience tuples\n",
    "        idxs = np.arange(sampling_probs.size)\n",
    "        random_idxs = self._random_state.choice(idxs,\n",
    "                                                size=batch_size,\n",
    "                                                replace=True,\n",
    "                                                p=sampling_probs)\n",
    "        \n",
    "        # select the experiences and sampling weights\n",
    "        sampled_experiences = self._buffer[\"experience\"][random_idxs]\n",
    "        sampled_weights = weights[random_idxs]\n",
    "        \n",
    "        return random_idxs, sampled_experiences, sampled_weights\n",
    "\n",
    "    def update_priorities(self, idxs: np.ndarray, priorities: np.ndarray) -> None:\n",
    "        \"\"\"Update the priorities associated with particular experiences.\"\"\"\n",
    "        self._buffer[\"priority\"][idxs] = priorities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning algorithm\n",
    "\n",
    "All that remains is to give a rough sketch of the algorithm that I use to train my agent. The \n",
    "following is Python pseudo-code for the double deep Q-Network (DDQN) algorithm with prioritized \n",
    "experience.\n",
    "\n",
    "```python\n",
    "# initialize the UnityEnvironment\n",
    "env = UnityEnvironment()\n",
    "\n",
    "# hyper-parameters\n",
    "batch_size = 32 # number of experience tuples used per gradient descent parameter update.\n",
    "buffer_size = 10000 # number of experience tuples stored in the replay buffer\n",
    "gamma = 0.99 # discount factor used in the Q-learning update\n",
    "target_network_update_frequency = 4 # measured in number of parameter updates\n",
    "update_frequency = 4 # measured in number of timesteps\n",
    "\n",
    "# initializing the various data structures\n",
    "replay_buffer = PrioritizedExperienceReplayBuffer(maximum_size,\n",
    "                                                  alpha,\n",
    "                                                  beta,\n",
    "                                                  beta_annealing_schedule)\n",
    "online_q_network = initialize_q_network()\n",
    "target_q_network = initialize_q_network()\n",
    "synchronize_q_networks(target_q_network, online_q_network)\n",
    "\n",
    "for i in range(number_episodes)\n",
    "\n",
    "    # initialize the environment state\n",
    "    state = env.reset()\n",
    "\n",
    "    # simulate a single training episode\n",
    "    done = False\n",
    "    timesteps = 0\n",
    "    parameter_updates = 0\n",
    "    while not done:\n",
    "\n",
    "        # greedy action based on Q(s, a; theta)\n",
    "        epsilon = epsilon_decay_schedule(i)\n",
    "        action = select_epsilon_greedy_action(state, online_q_network, epsilon) \n",
    "\n",
    "        # update the environment based on the chosen action\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # agent records experience in its replay buffer\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        replay_buffer.append(experience)\n",
    "\n",
    "        # agent samples a mini-batch of experiences from its replay buffer\n",
    "        experiences, sampling_weights = replay_buffer.sample()\n",
    "        \n",
    "        # preprocess experiences (if necessary)\n",
    "        preprocessed_experiences = preprocess_fn(experiences)\n",
    "        states, actions, rewards, next_states, dones = preprocessed_experiences\n",
    "\n",
    "        # agent learns every update_frequency timesteps\n",
    "        if timesteps % update_frequency == 0:\n",
    "            \n",
    "            # compute the temporal difference errors\n",
    "            td_errors = double_q_learning_error(states,\n",
    "                                                actions,\n",
    "                                                rewards,\n",
    "                                                next_states,\n",
    "                                                dones,\n",
    "                                                gamma,\n",
    "                                                online_q_network,\n",
    "                                                target_q_network)\n",
    "            \n",
    "            # update priorities associated with mini-batch experiences\n",
    "            new_priorities = td_errors.abs()\n",
    "            replay_buffer.update_priorities(new_priorities, experiences)\n",
    "\n",
    "            # agent updates the parameters theta using gradient descent\n",
    "            loss = mean_squared_error(td_errors, sampling_weights)\n",
    "            gradient_descent_update(loss)\n",
    "            \n",
    "            parameter_updates += 1\n",
    "\n",
    "        # every target_network_update_frequency timesteps set theta^- = theta\n",
    "        if parameter_updates % target_network_update_frequency == 0:\n",
    "            synchronize_q_networks(target_q_network, online_q_network)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My implementation of the DDQN algorithm with prioritized experience replay can be found in [`src/agents.py`](./src/agents.py) and [`src/utils.py`](./src/utils.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of the rewards\n",
    "\n",
    "I used [Pandas](https://pandas.pydata.org/) to quickly plot the time series of scores along with a 100 episode moving average. This particular training run solved the task in 976 episodes. The degree of prioritization given to experiences, $\\alpha$, was set to 0.5, the strength of importance sampling, $\\beta$, was annealed exponentially from an initial value of 0 to 1 over the course of training, and the discount factor, $\\gamma$, was set to 0.95 (the complete set of hyperparameters can be found in the [`notebooks/Navigation.ipynb`](notebooks/Navigation.ipynb). Using this set of hyperparameters my RL agent typically solves the task in 800-1000 training episodes.\n",
    "\n",
    "<center>\n",
    "    <img src=assets/rewards-plot.jpg>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model weights\n",
    "\n",
    "The model weights are saved in the `assets/top-score-checkpoint.pth` file. The file contains a pickled Python dictionary with keys `q-network-state` containing the model weights and `optimizer-state` containing the state of the Adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for future work?\n",
    "\n",
    "I have a few ideas for future work. Perhaps the most straight forward thing to do would be to \n",
    "continue following the literature and the algorithmic improvements from \n",
    "[*Dueling Network Architectures for Deep Reinforcement Learning*](https://arxiv.org/abs/1511.06581),\n",
    "[*Noisy Networks for Exploration*](https://arxiv.org/abs/1706.10295), and perhaps \n",
    "[*Rainbow: Combining Improvements in Deep Reinforcement Learning*](https://arxiv.org/abs/1710.02298). \n",
    "\n",
    "However, I am interested in reinforcement learning directly from pixel data where the action space \n",
    "is continuous. To tackle these more challenging tasks it might make sense to jump ahead and work on \n",
    "implementing more recent approaches such as \n",
    "[*Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor*](https://arxiv.org/abs/1801.01290) and \n",
    "[*Image Augmentation is All You Need: Regularizing Deep Reinforcement Learnging from Pixels](https://arxiv.org/pdf/2004.13649.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
